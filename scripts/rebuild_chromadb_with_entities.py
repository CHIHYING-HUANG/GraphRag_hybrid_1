"""
å¿«é€Ÿé‡å»º ChromaDB (å« entities from Neo4j)

ä¸éœ€è¦é‡æ–°æå–å¯¦é«”ï¼Œç›´æ¥å¾ Neo4j è®€å–ç¾æœ‰ entities
"""

import sys
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import json
import logging
from langchain_core.documents import Document

from app.database.vector_store import VectorStoreManager
from app.database.graph_store import GraphStoreManager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def rebuild_chromadb_with_entities(limit: int = None):
    """
    é‡å»º ChromaDBï¼Œä¸¦å¾ Neo4j å–å¾— entities åŠ å…¥ metadata
    
    æ­¥é©Ÿ:
    1. è®€å– corpus.json
    2. å¾ Neo4j æŸ¥è©¢æ¯å€‹æ–‡æª”çš„ entities
    3. å»ºç«‹ Document (å« entities metadata)
    4. å­˜å…¥ ChromaDB
    """
    
    # åˆå§‹åŒ–
    vector_store = VectorStoreManager()
    graph_store = GraphStoreManager()
    
    # 1. è®€å– corpus
    logger.info("ğŸ“– è®€å– corpus.json...")
    with open("data/corpus.json", "r", encoding="utf-8") as f:
        corpus = json.load(f)
    
    if limit:
        corpus = corpus[:limit]
    
    logger.info(f"ç¸½å…± {len(corpus)} ç¯‡æ–‡æª”")
    
    # 2. å»ºç«‹ Documentsï¼ˆå« entitiesï¼‰
    documents = []
    ids = []
    
    for i, doc_data in enumerate(corpus, 1):
        doc_id = doc_data.get("doc_id")
        content = doc_data.get("content", "")
        original_source = doc_data.get("original_source", "")
        
        if not content or not doc_id:
            continue
        
        # ğŸ” å¾ Neo4j æŸ¥è©¢æ­¤æ–‡æª”çš„ entities
        try:
            query = """
            MATCH (e:Entity)
            WHERE e.doc_id = $doc_id
            RETURN e.name AS entity
            """
            results = graph_store.graph.query(query, {"doc_id": doc_id})
            entities = [record["entity"] for record in results]
        except Exception as e:
            logger.warning(f"æŸ¥è©¢ entities å¤±æ•— (doc {doc_id[:20]}...): {e}")
            entities = []
        
        # å»ºç«‹ Document
        doc = Document(
            page_content=content,
            metadata={
                "doc_id": doc_id,
                "original_source": original_source,
                "entities": ",".join(entities) if entities else ""  # âœ… è½‰æˆå­—ä¸²
            }
        )
        
        documents.append(doc)
        ids.append(doc_id)
        
        if i % 10 == 0:
            logger.info(f"è™•ç†é€²åº¦: {i}/{len(corpus)} - Doc {doc_id[:20]}... æœ‰ {len(entities)} å€‹ entities")
    
    # 3. å­˜å…¥ ChromaDB
    logger.info(f"ğŸ’¾ å­˜å…¥ ChromaDB ({len(documents)} ç¯‡æ–‡æª”)...")
    vector_store.add_documents(documents, ids=ids)
    
    logger.info("âœ… å®Œæˆï¼ChromaDB å·²é‡å»ºï¼Œmetadata å« entities")
    
    # 4. é©—è­‰
    logger.info("\nğŸ” é©—è­‰çµæœ:")
    stats = vector_store.get_stats()
    logger.info(f"  - ç¸½æ–‡æª”æ•¸: {stats['total_documents']}")
    
    # éš¨æ©Ÿæª¢æŸ¥ä¸€ç¯‡
    sample_result = vector_store.collection.get(limit=1, include=['metadatas'])
    if sample_result['metadatas']:
        sample_meta = sample_result['metadatas'][0]
        logger.info(f"\nğŸ“‹ ç¯„ä¾‹æ–‡æª”:")
        logger.info(f"  - doc_id: {sample_meta.get('doc_id', 'N/A')[:30]}...")
        logger.info(f"  - entities: {sample_meta.get('entities', [])[:5]}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="é‡å»º ChromaDB (å« Neo4j entities)")
    parser.add_argument("--limit", type=int, help="è™•ç†çš„æ–‡æª”æ•¸é‡")
    args = parser.parse_args()
    
    logger.info("=" * 70)
    logger.info("ğŸ”§ ChromaDB é‡å»ºå·¥å…· (å¾ Neo4j å–å¾— entities)")
    logger.info("=" * 70)
    logger.info("âš ï¸  æ³¨æ„: æ­¤è…³æœ¬æœƒæ¸…ç©ºä¸¦é‡å»º ChromaDB")
    logger.info("â„¹ï¸  Neo4j ä¸æœƒè¢«ä¿®æ”¹")
    logger.info("=" * 70)
    
    input("\næŒ‰ Enter ç¹¼çºŒï¼Œæˆ– Ctrl+C å–æ¶ˆ...")
    
    rebuild_chromadb_with_entities(args.limit)
