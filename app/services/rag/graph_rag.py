"""
Graph RAG å·¥ä½œæµç¨‹ï¼ˆLangGraph æ¶æ§‹ï¼‰

ä½¿ç”¨ LangGraph StateGraph å¯¦ä½œæ··åˆæª¢ç´¢ï¼ˆå‘é‡ + åœ–è­œï¼‰çš„ Graph RAG ç³»çµ±
"""

import logging
from typing import List, Dict, Any, TypedDict

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END

from app.database.vector_store import VectorStoreManager
from app.database.graph_store import GraphStoreManager
from app.services.retrieval.vector_retriever import VectorRetriever
from app.services.retrieval.graph_retriever import GraphRetriever
from app.core.config import settings

logger = logging.getLogger(__name__)


# =============================================================================
# Graph State å®šç¾©
# =============================================================================

class GraphState(TypedDict):
    """
    Graph RAG å·¥ä½œæµç¨‹çš„ç‹€æ…‹
    
    å±¬æ€§ï¼š
        question: ç”¨æˆ¶å•é¡Œ
        expanded_queries: æŸ¥è©¢æ“´å±•å¾Œçš„å¤šå€‹å•é¡Œè®Šé«”
        candidates: é‡æ’åºå‰çš„å€™é¸æ–‡æª”åˆ—è¡¨ (ç”¨æ–¼ reranking)
        vector_context: å‘é‡æª¢ç´¢çš„çµæœ
        graph_context: åœ–è­œæª¢ç´¢çš„çµæœ
        final_answer: æœ€çµ‚ç”Ÿæˆçš„ç­”æ¡ˆ
    """
    question: str
    expanded_queries: List[str]
    candidates: List[Dict[str, Any]]
    vector_context: List[str]
    graph_context: List[str]
    final_answer: str
    retrieved_doc_ids: List[str]  # æœ€çµ‚æª¢ç´¢åˆ°çš„æ–‡æª” ID (ç”¨æ–¼è©•ä¼°)


# =============================================================================
# å…¨åŸŸå¯¦ä¾‹ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰
# =============================================================================

_vector_store = None
_graph_store = None
_vector_retriever = None
_graph_retriever = None
_llm = None


def _get_vector_retriever() -> VectorRetriever:
    """ç²å–å‘é‡æª¢ç´¢å™¨ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _vector_store, _vector_retriever
    if _vector_retriever is None:
        _vector_store = VectorStoreManager()
        _vector_retriever = VectorRetriever(_vector_store)
    return _vector_retriever


def _get_graph_retriever() -> GraphRetriever:
    """ç²å–åœ–è­œæª¢ç´¢å™¨ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _graph_store, _graph_retriever
    if _graph_retriever is None:
        _graph_store = GraphStoreManager()
        _graph_retriever = GraphRetriever(_graph_store)
    return _graph_retriever


def _get_llm() -> ChatOpenAI:
    """ç²å– LLMï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _llm
    if _llm is None:
        _llm = ChatOpenAI(
            model=settings.LLM_MODEL,
            temperature=0,
            openai_api_key=settings.OPENAI_API_KEY
        )
    return _llm


# =============================================================================
# LangGraph ç¯€é»å‡½æ•¸
# =============================================================================

def query_expansion_node(state: GraphState) -> GraphState:
    """
    ç¯€é» 1: æŸ¥è©¢æ“´å±•
    
    ä½¿ç”¨ LLM å°‡åŸå§‹å•é¡Œæ”¹å¯«æˆ 3 å€‹èªæ„ç›¸ä¼¼çš„è®Šé«”ï¼Œå¢åŠ æª¢ç´¢å¬å›ç‡
    """
    question = state["question"]
    
    try:
        expansion_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯æŸ¥è©¢å„ªåŒ–å°ˆå®¶,æ“…é•·åˆ†è§£å¤šæ­¥é©Ÿå•é¡Œã€‚

ä»»å‹™:
1. è­˜åˆ¥å•é¡Œä¸­çš„æ ¸å¿ƒå¯¦é«”å’Œé—œä¿‚
2. ç”¢ç”Ÿ 3 å€‹æœ‰åŠ©æ–¼æ‰¾åˆ°ç›¸é—œæ–‡æª”çš„æŸ¥è©¢è®Šé«”
3. å¦‚æœæ˜¯å¤šæ­¥é©Ÿå•é¡Œ,è€ƒæ…®æ‹†è§£ä¸­é–“æ­¥é©Ÿ

è¦æ±‚:
- è®Šé«”æ‡‰æ¶µè“‹ä¸åŒè§’åº¦æˆ–ä¸­é–“æ­¥é©Ÿ
- ä½¿ç”¨åŒç¾©è©ã€å¯¦é«”åˆ¥å
- æ¯å€‹å•é¡Œç”¨æ›è¡Œåˆ†éš”ï¼Œä¸è¦ç·¨è™Ÿ

ç¯„ä¾‹ (å¤šè·³é¡Œ):
åŸå§‹å•é¡Œ: Açš„çˆ¶è¦ªæ˜¯åœ¨å“ªä¸€å¹´å‡ºç”Ÿçš„ï¼Ÿ
è®Šé«”å•é¡Œ:
Açš„çˆ¶è¦ªæ˜¯èª°
Açš„çˆ¶è¦ªå‡ºç”Ÿæ—¥æœŸ
Açš„å®¶æ—èƒŒæ™¯

åŸå§‹å•é¡Œ: Båœ‹çš„é¦–éƒ½åœ¨å“ªï¼Ÿ
è®Šé«”å•é¡Œ:
Båœ‹çš„è¡Œæ”¿ä¸­å¿ƒä½æ–¼ä½•è™•
Båœ‹é¦–éƒ½åç¨±
Båœ‹æ”¿åºœæ‰€åœ¨åœ°
"""),
            ("human", "åŸå§‹å•é¡Œï¼š{question}")
        ])
        
        llm = _get_llm()
        chain = expansion_prompt | llm
        
        # å–å¾— Langfuse callback
        from app.core.langfuse_helper import get_callbacks
        callbacks = get_callbacks()
        
        response = chain.invoke(
            {"question": question},
            config={"callbacks": callbacks} if callbacks else {}
        )
        
        # è§£ææ“´å±•å¾Œçš„æŸ¥è©¢ï¼ˆåŒ…å«åŸå§‹å•é¡Œï¼‰
        expanded = [question]  # ä¿ç•™åŸå§‹å•é¡Œ
        for line in response.content.strip().split("\n"):
            line = line.strip()
            if line and line not in expanded:
                expanded.append(line)
        
        logger.debug(f"æŸ¥è©¢æ“´å±•ï¼š{len(expanded)} å€‹è®Šé«”")
        return {"expanded_queries": expanded[:3]}  # æœ€å¤š 3 å€‹
        
    except Exception as e:
        logger.error(f"æŸ¥è©¢æ“´å±•éŒ¯èª¤ï¼š{e}")
        return {"expanded_queries": [question]}  # fallback to original


def retrieve_vector_node(state: GraphState) -> GraphState:
    """
    ç¯€é» 2ï¼šå¤šæŸ¥è©¢å‘é‡æª¢ç´¢ + åœ–è­œæ“´å±•
    
    ä½¿ç”¨æ“´å±•å¾Œçš„æŸ¥è©¢å¾ ChromaDB æª¢ç´¢å€™é¸æ–‡æª”,ä¸¦åˆ©ç”¨åœ–è­œé—œä¿‚ç™¼ç¾é¡å¤–ç›¸é—œæ–‡æª”
    """
    expanded_queries = state.get("expanded_queries", [state["question"]])
    
    try:
        vector_retriever = _get_vector_retriever()
        graph_retriever = _get_graph_retriever()
        
        # Step 1: å°æ¯å€‹æ“´å±•æŸ¥è©¢é€²è¡Œå‘é‡æª¢ç´¢
        all_candidates = []
        seen_ids = set()
        initial_entities = set()  # æ”¶é›†åˆå§‹æª¢ç´¢çµæœä¸­çš„å¯¦é«”
        
        for query in expanded_queries:
            docs_with_meta = vector_retriever.retrieve_with_metadata(query, k=30)
            
            for doc_meta in docs_with_meta:
                doc_id = doc_meta.get("metadata", {}).get("doc_id")
                if doc_id and doc_id not in seen_ids:
                    all_candidates.append({
                        "doc_id": doc_id,
                        "content": doc_meta.get("content", ""),
                        "metadata": doc_meta.get("metadata", {})
                    })
                    seen_ids.add(doc_id)
                    
                    # å¾æ–‡æª”çš„ entities metadata ä¸­æ”¶é›†å¯¦é«”
                    entities = doc_meta.get("metadata", {}).get("entities", [])
                    if isinstance(entities, list) and entities:
                        initial_entities.update(entities[:3])  # æ¯ç¯‡æ–‡æª”å–å‰3å€‹å¯¦é«”
                    else:
                        # ğŸ”§ Fallback: metadata æ²’æœ‰ entities æ™‚ï¼Œå¾å…§å®¹ç²—ç•¥æå–
                        content = doc_meta.get("content", "")
                        if content:
                            import re
                            # æ‰¾ä¸­æ–‡äººå/åœ°åï¼ˆå¸¸è¦‹å§“æ°é–‹é ­ï¼‰å’Œè‹±æ–‡å°ˆæœ‰åè©
                            chinese_names = re.findall(r'[\u4e00-\u9fa5]{2,4}(?:å…ˆç”Ÿ|å¥³å£«|æ•™æˆ|åšå£«|ç¸½çµ±|ä¸»å¸­)?', content[:800])
                            # è‹±æ–‡å°ˆæœ‰åè©ï¼ˆå¤§å¯«é–‹é ­çš„é€£çºŒè©ï¼‰
                            english_names = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,3}\b', content[:800])
                            
                            potential_entities = list(set(chinese_names[:5] + english_names[:5]))
                            if potential_entities:
                                initial_entities.update(potential_entities[:3])
        
        logger.debug(f"åˆå§‹å‘é‡æª¢ç´¢: {len(all_candidates)} ç¯‡æ–‡æª”, {len(initial_entities)} å€‹å¯¦é«”")
        
        # Step 2: åœ–è­œæ“´å±• - æ‰¾å‡ºç›¸é—œå¯¦é«”
        if initial_entities:
            import asyncio
            related_entities = asyncio.run(
                graph_retriever.get_related_entities(
                    list(initial_entities)[:5],  # é™åˆ¶èµ·å§‹å¯¦é«”æ•¸é‡
                    max_neighbors=3
                )
            )
            
            print(f"ğŸš€ [Graph Expansion] ç™¼ç¾ {len(related_entities)} å€‹ç›¸é—œå¯¦é«”: {related_entities}")
            logger.debug(f"åœ–è­œæ“´å±•ç™¼ç¾ {len(related_entities)} å€‹ç›¸é—œå¯¦é«”")
            
            # Step 3: ç”¨ç›¸é—œå¯¦é«”åšé¡å¤–æª¢ç´¢
            for entity in related_entities[:5]:  # é™åˆ¶æ“´å±•å¯¦é«”æ•¸é‡
                entity_docs = vector_retriever.retrieve_with_metadata(entity, k=2)  # æ¯å€‹å¯¦é«”å–2ç¯‡
                
                for doc_meta in entity_docs:
                    doc_id = doc_meta.get("metadata", {}).get("doc_id")
                    if doc_id and doc_id not in seen_ids:
                        all_candidates.append({
                            "doc_id": doc_id,
                            "content": doc_meta.get("content", ""),
                            "metadata": doc_meta.get("metadata", {})
                        })
                        seen_ids.add(doc_id)
            
            logger.debug(f"åœ–è­œæ“´å±•å¾Œç¸½å…± {len(all_candidates)} ç¯‡å€™é¸æ–‡æª”")
        
        # é™åˆ¶å€™é¸æ•¸é‡
        return {"candidates": all_candidates[:30]}
        
    except Exception as e:
        logger.error(f"æª¢ç´¢éŒ¯èª¤ï¼š{e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"candidates": []}


def rerank_node(state: GraphState) -> GraphState:
    """
    ç¯€é» 3ï¼šLLM é‡æ’åº
    
    ç¯€é» 3ï¼šLLM é‡æ’åº
    
    ä½¿ç”¨ LLM å°å€™é¸æ–‡æª”é€²è¡Œé‡æ’åºï¼Œé¸å‡ºæœ€ç›¸é—œçš„ Top-5
    """
    question = state["question"]
    candidates = state.get("candidates", [])
    
    if not candidates:
        return {"vector_context": []}
    
    try:
        # æ§‹å»ºé‡æ’åºæç¤ºè© (Listwise Reranking)
        candidate_texts = []
        for idx, cand in enumerate(candidates[:30], 1):  # å¢åŠ åˆ°30æå‡è¤‡é›œå•é¡Œè¦†è“‹ç‡  # é™åˆ¶æœ€å¤š 20 å€‹å€™é¸
            content_preview = cand["content"][:300]  # åªå–å‰ 300 å­—ç¬¦ä»¥ç¯€çœ token
            candidate_texts.append(f"[{idx}] {content_preview}...")
        
        candidates_str = "\n\n".join(candidate_texts)
        
        rerank_prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€å€‹æ–‡æª”æ’åºå°ˆå®¶ã€‚è«‹æ ¹æ“šå•é¡Œï¼Œå¾å€™é¸æ–‡æª”ä¸­é¸å‡ºæœ€ç›¸é—œçš„ 5 ç¯‡æ–‡æª”ã€‚åªéœ€è¿”å›æ–‡æª”ç·¨è™Ÿï¼ˆä¾‹å¦‚ï¼š1,3,5,8,12ï¼‰ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œä¸è¦å…¶ä»–èªªæ˜ã€‚"),
            ("human", "å•é¡Œï¼š{question}\n\nå€™é¸æ–‡æª”ï¼š\n{candidates}")
        ])
        
        llm = _get_llm()
        chain = rerank_prompt | llm
        
        # å–å¾— Langfuse callback
        from app.core.langfuse_helper import get_callbacks
        callbacks = get_callbacks()
        
        response = chain.invoke(
            {
                "question": question,
                "candidates": candidates_str
            },
            config={"callbacks": callbacks} if callbacks else {}
        )
        
        # è§£ææ’åºçµæœ
        # è§£ææ’åºçµæœ (ä½¿ç”¨ Regex å¢å¼·é­¯æ£’æ€§)
        import re
        selected_indices = []
        # æ‰¾å‡ºæ‰€æœ‰æ•¸å­—
        found_numbers = re.findall(r'\d+', response.content)
        
        for num_str in found_numbers:
            try:
                idx = int(num_str)
                if 1 <= idx <= len(candidates):
                    selected_indices.append(idx - 1)  # è½‰ç‚º 0-index
            except ValueError:
                continue
        
        # å–å¾—é‡æ’åºå¾Œçš„æ–‡æª”å…§å®¹ å’Œ ID
        reranked_contents = []
        reranked_ids = []
        for idx in selected_indices[:5]:  # Top-5
            if 0 <= idx < len(candidates):
                reranked_contents.append(candidates[idx]["content"])
                reranked_ids.append(candidates[idx].get("doc_id"))
        
        # Fallback: å¦‚æœ LLM æ²’æœ‰è¿”å›åˆæ³•ç´¢å¼•ï¼Œä½¿ç”¨å‰ 5 å€‹
        if not reranked_contents and candidates:
            reranked_contents = [c["content"] for c in candidates[:5]]
            reranked_ids = [c.get("doc_id") for c in candidates[:5]]
        
        # éæ¿¾ None ID
        reranked_ids = [uid for uid in reranked_ids if uid]
        
        logger.debug(f"é‡æ’åºå¾Œé¸å‡º {len(reranked_contents)} ç¯‡æ–‡æª”")
        return {"vector_context": reranked_contents, "retrieved_doc_ids": reranked_ids}
        
    except Exception as e:
        logger.error(f"é‡æ’åºéŒ¯èª¤ï¼š{e}")
        # Fallback: ç›´æ¥ä½¿ç”¨å‰ 5 å€‹å€™é¸
        fallback_contents = [c["content"] for c in candidates[:5]]
        fallback_ids = [c.get("doc_id") for c in candidates[:5]]
        return {"vector_context": fallback_contents, "retrieved_doc_ids": [uid for uid in fallback_ids if uid]}


async def retrieve_graph_node(state: GraphState) -> GraphState:
    """
    ç¯€é» 4ï¼šåœ–è­œæª¢ç´¢
    
    å¾ Neo4j çŸ¥è­˜åœ–è­œæª¢ç´¢ç›¸é—œå¯¦é«”å’Œé—œä¿‚
    """
    question = state["question"]
    
    try:
        graph_retriever = _get_graph_retriever()
        graph_context = await graph_retriever.retrieve(
            question,
            max_entities=3,
            max_relations_per_entity=10  # 5â†’10 æä¾›æ›´å¤šé—œä¿‚è·¯å¾‘
        )
        
        logger.debug(f"åœ–è­œæª¢ç´¢åˆ° {len(graph_context)} æ¢é—œä¿‚")
        return {"graph_context": graph_context}
        
    except Exception as e:
        logger.warning(f"åœ–è­œæª¢ç´¢éŒ¯èª¤ï¼š{e}")
        return {"graph_context": []}


def generate_answer_node(state: GraphState) -> GraphState:
    """
    ç¯€é» 5ï¼šç­”æ¡ˆç”Ÿæˆ
    
    åŸºæ–¼å‘é‡å’Œåœ–è­œæª¢ç´¢çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
    """
    question = state["question"]
    vector_context = state.get("vector_context", [])
    graph_context = state.get("graph_context", [])
    
    try:
        # æ•´åˆä¸Šä¸‹æ–‡
        context_parts = []
        
        if vector_context:
            context_parts.append("ã€å‘é‡æª¢ç´¢ä¸Šä¸‹æ–‡ã€‘\n" + "\n\n".join(vector_context))
        
        if graph_context:
            context_parts.append("ã€åœ–è­œæª¢ç´¢ä¸Šä¸‹æ–‡ã€‘\n" + "\n".join(graph_context))
        
        context_str = "\n\n".join(context_parts) if context_parts else "ç„¡ç›¸é—œä¸Šä¸‹æ–‡"
        
        # ç”Ÿæˆç­”æ¡ˆ (å„ªåŒ–çš„å¤šæ­¥æ¨ç† Prompt - æ”¯æ´è¤‡é›œå¤šè·³èˆ‡æ•¸å­—æ¨ç† + Chain of Thought)
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å•ç­”åŠ©æ‰‹,æ“…é•·æ•´åˆå¤šç¯‡æ–‡æª”è³‡è¨Šä¸¦é€²è¡Œå¤šæ­¥é©Ÿé‚è¼¯æ¨ç†ã€‚

                å›ç­”ç­–ç•¥:
                1. å…ˆåˆ†æå•é¡Œçµæ§‹,è­˜åˆ¥éœ€è¦å¹¾å€‹æ­¥é©Ÿ
                2. å°æ–¼å¤šæ­¥é©Ÿå•é¡Œ,ä¾åºå®Œæˆæ¯å€‹æ­¥é©Ÿ:
                   - ç¬¬ 1 æ­¥: å¾æ–‡æª”ä¸­æ‰¾åˆ°èµ·é»è³‡è¨Š (å¦‚äººåã€å¯¦é«”)ã€‚**æ³¨æ„ï¼šå¿…é ˆç²¾ç¢ºåŒ¹é…å¯¦é«”åç¨±ï¼Œé¿å…æ··æ·†åŒåæˆ–ç›¸ä¼¼å¯¦é«”ã€‚**
                   - ç¬¬ 2 æ­¥: ç”¨ç¬¬ 1 æ­¥çš„çµæœåœ¨æ–‡æª”ä¸­æ‰¾ä¸­é–“è³‡è¨Š (å¦‚é—œä¿‚ã€å±¬æ€§)
                   - ç¬¬ 3 æ­¥: ç”¨ç¬¬ 2 æ­¥çš„çµæœæ‰¾æœ€çµ‚ç­”æ¡ˆ
                3. å°æ–¼æ¶‰åŠæ•¸å€¼æˆ–æ¯”ä¾‹çš„å•é¡Œ:
                   - **å„ªå…ˆå°‹æ‰¾ç›´æ¥ç­”æ¡ˆ**ï¼šå¦‚æœæ–‡æª”ä¸­ç›´æ¥æä¾›äº†æ•¸å€¼æˆ–æ™‚é–“é•·åº¦ï¼ˆå¦‚ã€Œæ­·æ™‚83å¹´ã€ã€ã€Œä½”æ¯”20%ã€ï¼‰ï¼Œ**å¿…é ˆç›´æ¥å¼•ç”¨**ï¼Œç¦æ­¢è‡ªè¡Œè¨ˆç®—ã€‚
                   - åªæœ‰åœ¨æ–‡æª”æœªç›´æ¥æä¾›ç­”æ¡ˆæ™‚ï¼Œæ‰æ ¹æ“šæ–‡æª”ä¸­çš„æ•¸æ“šé€²è¡Œè¨ˆç®—ã€‚
                   - è‹¥æ–‡æª”æä¾›çš„æ•¸æ“šèˆ‡å•é¡Œè©¢å•çš„è§’åº¦ç›¸åï¼Œè«‹é€²è¡Œç°¡å–®çš„æ•¸å­¸è½‰æ› (å¦‚ 100% - X%) ä»¥é©—è­‰ç­”æ¡ˆã€‚
                4. **é‚è¼¯ä¸€è‡´æ€§æª¢æŸ¥**ï¼š
                   - å°æ–¼æ˜¯éé¡Œï¼ˆYes/Noï¼‰ï¼Œç¢ºä¿ä½ çš„çµè«–ï¼ˆæ˜¯/å¦ï¼‰èˆ‡ä½ çš„è§£é‡‹å®Œå…¨ä¸€è‡´ã€‚
                   - ä¾‹å¦‚ï¼šå¦‚æœè§£é‡‹æ˜¯ã€ŒAä¾†è‡ªå¾·åœ‹ï¼ŒBä¾†è‡ªç¾åœ‹ã€ï¼Œçµè«–å¿…é ˆæ˜¯ã€Œä¸æ˜¯ã€ï¼ˆä¾†è‡ªä¸åŒåœ‹å®¶ï¼‰ã€‚
                5. æ•´åˆæ‰€æœ‰è³‡è¨Šå¾—å‡ºçµè«–

                é‡è¦åŸå‰‡:
                - **åš´æ ¼ä»¥æ–‡æª”ç‚ºæº–**ï¼šå¦‚æœæ–‡æª”ä¸­æœ‰æ˜ç¢ºè³‡è¨Šï¼Œå¿…é ˆå„ªå…ˆä½¿ç”¨æ–‡æª”å…§å®¹ï¼Œè€Œéé è¨“ç·´çŸ¥è­˜ï¼ˆä¾‹å¦‚ä¸è¦ä½¿ç”¨å¤–éƒ¨çŸ¥è­˜è£œå……å¹´ä»½ï¼‰
                - å³ä½¿è³‡è¨Šåˆ†æ•£åœ¨ 2-4 ç¯‡ä¸åŒæ–‡æª”,ä¹Ÿè¦åŠªåŠ›æ•´åˆ
                - å°æ–¼é—œä¿‚é¡å•é¡Œ (å¦‚"ç¹¼çˆ¶"),æ˜ç¢ºæ¨ç†é—œä¿‚éˆ
                - é‡åˆ°æ•¸å­—å•é¡Œ,è«‹ç²¾ç¢ºæ ¸å°æ–‡æª”ä¸­çš„æ•¸æ“š,ä¸è¦æ†‘å°è±¡å›ç­”
                - åªæœ‰åœ¨ä¸Šä¸‹æ–‡å®Œå…¨æ²’æœ‰ç›¸é—œè³‡è¨Šæ™‚,æ‰å›ç­”ã€Œæ ¹æ“šæä¾›çš„è³‡æ–™ç„¡æ³•å›ç­”æ­¤å•é¡Œã€

                **è¼¸å‡ºæ ¼å¼è¦æ±‚**ï¼š
                è«‹å‹™å¿…æŒ‰ç…§ä»¥ä¸‹ XML æ ¼å¼è¼¸å‡ºä½ çš„æ€è€ƒéç¨‹å’Œæœ€çµ‚ç­”æ¡ˆï¼š
                <reasoning>
                é€™è£¡å¯«ä¸‹ä½ çš„é€æ­¥æ¨ç†éç¨‹...
                1. æ ¹æ“šæ–‡æª”X...
                2. ç™¼ç¾...
                3. å› æ­¤...
                </reasoning>
                <answer>
                é€™è£¡å¯«ä¸‹æœ€çµ‚ç­”æ¡ˆï¼ˆç²¾ç°¡ã€ç›´æ¥ï¼‰
                </answer>

                ç¯„ä¾‹åƒè€ƒ:
                
                ç¯„ä¾‹1 - å„ªå…ˆä½¿ç”¨æ–‡æª”æ•¸å€¼:
                Q: æŸæœä»£æŒçºŒäº†å¤šä¹…ï¼Ÿ
                Doc: "æŸæœä»£å»ºç«‹æ–¼100å¹´ï¼Œæ­·æ™‚300å¹´ï¼Œæ–¼400å¹´æ»…äº¡ã€‚"
                <reasoning>
                1. æ–‡æª”æ˜ç¢ºæåˆ°ã€Œæ­·æ™‚300å¹´ã€ã€‚
                2. é›–ç„¶400-100=300ï¼Œä½†æ–‡æª”å·²æœ‰ç›´æ¥ç­”æ¡ˆã€‚
                </reasoning>
                <answer>
                300å¹´
                </answer>
                
                ç¯„ä¾‹2 - æ•¸å€¼æ¯”è¼ƒ (é€šç”¨é‚è¼¯):
                Q: è˜‹æœå’Œæ©˜å­å“ªä¸€å€‹æ¯”è¼ƒé‡ï¼Ÿ
                Doc: "è˜‹æœé‡200å…‹ï¼Œæ©˜å­é‡150å…‹ã€‚"
                <reasoning>
                1. æ–‡æª”æŒ‡å‡ºè˜‹æœé‡200å…‹ã€‚
                2. æ–‡æª”æŒ‡å‡ºæ©˜å­é‡150å…‹ã€‚
                3. 200å…‹ > 150å…‹ï¼Œæ‰€ä»¥è˜‹æœæ¯”è¼ƒé‡ã€‚
                </reasoning>
                <answer>
                è˜‹æœ
                </answer>
                
                ç¯„ä¾‹3 - è·¨æ–‡æª”æ¨ç† (åœ°ç†/æ©Ÿæ§‹):
                Q: Alphaå…¬å¸çš„ç¸½éƒ¨æ‰€åœ¨çš„åŸå¸‚ï¼Œå…¶å¸‚é•·æ˜¯èª°ï¼Ÿ
                Doc1: "Alphaå…¬å¸çš„ç¸½éƒ¨ä½æ–¼è²å…‹å¸‚ã€‚"
                Doc2: "è²å…‹å¸‚çš„å¸‚é•·æ˜¯è©¹å§†æ–¯Â·å²å¯†æ–¯ã€‚"
                <reasoning>
                1. å¾Doc1å¾—çŸ¥Alphaå…¬å¸ç¸½éƒ¨åœ¨è²å…‹å¸‚ã€‚
                2. å¾Doc2å¾—çŸ¥è²å…‹å¸‚å¸‚é•·æ˜¯è©¹å§†æ–¯Â·å²å¯†æ–¯ã€‚
                3. å› æ­¤ç­”æ¡ˆæ˜¯è©¹å§†æ–¯Â·å²å¯†æ–¯ã€‚
                </reasoning>
                <answer>
                è©¹å§†æ–¯Â·å²å¯†æ–¯
                </answer> 
                
                ç¯„ä¾‹4 - å¯¦é«”å€åˆ† (ç§‘å­¸/å®šç¾©):
                Q: ä»€éº¼æ˜¯ã€Œå…‰åˆä½œç”¨ã€çš„ä¸»è¦ç”¢ç‰©ï¼Ÿ
                Doc1: "å‘¼å¸ä½œç”¨ç”¢ç”ŸäºŒæ°§åŒ–ç¢³å’Œæ°´ã€‚"
                Doc2: "å…‰åˆä½œç”¨å°‡å…‰èƒ½è½‰åŒ–ç‚ºåŒ–å­¸èƒ½ï¼Œç”¢ç”Ÿè‘¡è„ç³–å’Œæ°§æ°£ã€‚"
                <reasoning>
                1. å•é¡Œè©¢å•å…‰åˆä½œç”¨çš„ç”¢ç‰©ã€‚
                2. Doc1æè¿°å‘¼å¸ä½œç”¨ï¼Œä¸ç›¸é—œã€‚
                3. Doc2æ˜ç¢ºæŒ‡å‡ºå…‰åˆä½œç”¨ç”¢ç”Ÿè‘¡è„ç³–å’Œæ°§æ°£ã€‚
                </reasoning>
                <answer>
                è‘¡è„ç³–å’Œæ°§æ°£
                </answer>"""),
            ("human", "ä¸Šä¸‹æ–‡:\n{context}\n\nå•é¡Œ:{question}\n\nè«‹ä¾ç…§æŒ‡å®šæ ¼å¼è¼¸å‡º:")
        ])
        
        llm = _get_llm()
        chain = prompt | llm
        
        # å–å¾— Langfuse callbackï¼ˆçµ±ä¸€ç®¡ç†ï¼‰
        from app.core.langfuse_helper import get_callbacks
        callbacks = get_callbacks()
        
        response = chain.invoke(
            {
                "context": context_str,
                "question": question
            },
            config={"callbacks": callbacks} if callbacks else {}
        )
        
        # è§£æè¼¸å‡ºï¼Œæå– <answer> æ¨™ç±¤å…§å®¹
        import re
        content = response.content
        final_answer = content
        
        match = re.search(r"<answer>(.*?)</answer>", content, re.DOTALL)
        if match:
            final_answer = match.group(1).strip()
        else:
            # Fallback: å¦‚æœæ²’æœ‰æ¨™ç±¤ï¼Œå˜—è©¦ç§»é™¤å¯èƒ½çš„ <reasoning> éƒ¨åˆ†
            reasoning_match = re.search(r"<reasoning>(.*?)</reasoning>", content, re.DOTALL)
            if reasoning_match:
                final_answer = content.replace(reasoning_match.group(0), "").strip()
            
        return {"final_answer": final_answer}
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆç­”æ¡ˆéŒ¯èª¤ï¼š{e}")
        return {"final_answer": "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"}


# =============================================================================
# å»ºç«‹ LangGraph å·¥ä½œæµç¨‹
# =============================================================================

def create_graph_rag_workflow() -> StateGraph:
    """
    å»ºç«‹ Graph RAG å·¥ä½œæµç¨‹ï¼ˆå„ªåŒ–ç‰ˆï¼‰
    
    å·¥ä½œæµç¨‹ï¼š
    1. query_expansionï¼šæŸ¥è©¢æ“´å±•ï¼ˆç”Ÿæˆ 3 å€‹å•é¡Œè®Šé«”ï¼‰
    2. retrieve_vectorï¼šå¤šæŸ¥è©¢å‘é‡æª¢ç´¢ï¼ˆTop-20 å€™é¸ï¼‰
    3. rerankï¼šLLM é‡æ’åºï¼ˆé¸å‡º Top-5ï¼‰
    4. retrieve_graphï¼šåœ–è­œæª¢ç´¢
    5. generate_answerï¼šç”Ÿæˆç­”æ¡ˆ
    
    å›å‚³ï¼š
        StateGraph: ç·¨è­¯å¾Œçš„å·¥ä½œæµç¨‹åœ–
    """
    # å»ºç«‹ç‹€æ…‹åœ–
    workflow = StateGraph(GraphState)
    
    # æ·»åŠ ç¯€é»
    workflow.add_node("query_expansion", query_expansion_node)
    workflow.add_node("retrieve_vector", retrieve_vector_node)
    workflow.add_node("rerank", rerank_node)
    workflow.add_node("retrieve_graph", retrieve_graph_node)
    workflow.add_node("generate_answer", generate_answer_node)
    
    # å®šç¾©é‚Šï¼ˆå·¥ä½œæµç¨‹ï¼‰
    workflow.set_entry_point("query_expansion")
    workflow.add_edge("query_expansion", "retrieve_vector")
    workflow.add_edge("retrieve_vector", "rerank")
    workflow.add_edge("rerank", "retrieve_graph")
    workflow.add_edge("retrieve_graph", "generate_answer")
    workflow.add_edge("generate_answer", END)
    
    # ç·¨è­¯åœ–
    return workflow.compile()


# å»ºç«‹å…¨åŸŸå·¥ä½œæµç¨‹å¯¦ä¾‹
_graph_rag_workflow = None


def _get_workflow():
    """ç²å–å·¥ä½œæµç¨‹å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _graph_rag_workflow
    if _graph_rag_workflow is None:
        _graph_rag_workflow = create_graph_rag_workflow()
    return _graph_rag_workflow


# =============================================================================
# ä¾¿æ·å‡½æ•¸
# =============================================================================

async def run_graph_rag(question: str) -> Dict[str, Any]:
    """
    åŸ·è¡Œå®Œæ•´çš„ Graph RAG æµç¨‹
    
   åƒæ•¸:
        question (str): ç”¨æˆ¶å•é¡Œ
    
    å›å‚³:
        Dict: åŒ…å«å•é¡Œã€ç­”æ¡ˆã€ä¸Šä¸‹æ–‡å’Œå»¶é²æŒ‡æ¨™çš„å­—å…¸
    """
    import time
    
    # è¨˜éŒ„é–‹å§‹æ™‚é–“
    start_time = time.time()
    
    # æº–å‚™åˆå§‹ç‹€æ…‹
    initial_state = {
        "question": question,
        "expanded_queries": [],
        "candidates": [],
        "vector_context": [],
        "graph_context": [],
        "final_answer": ""
    }
    
    # åŸ·è¡Œå·¥ä½œæµç¨‹ï¼ˆæª¢ç´¢éšæ®µï¼‰
    retrieval_start = time.time()
    workflow = _get_workflow()
    result = await workflow.ainvoke(initial_state)
    retrieval_time = time.time() - retrieval_start
    
    # ç”Ÿæˆéšæ®µçš„æ™‚é–“å·²åŒ…å«åœ¨ workflow ä¸­ï¼Œé€™è£¡è¨˜éŒ„ç¸½æ™‚é–“
    total_time = time.time() - start_time
    
    # ä¼°ç®—ç”Ÿæˆæ™‚é–“ï¼ˆç¸½æ™‚é–“ - æª¢ç´¢æ™‚é–“çš„åˆç†éƒ¨åˆ†ï¼‰
    # æ³¨æ„ï¼šé€™æ˜¯è¿‘ä¼¼å€¼ï¼Œå› ç‚ºå·¥ä½œæµåŒ…å«å¤šå€‹éšæ®µ
    generation_time = max(0, total_time - retrieval_time * 0.5)
    
    return {
        "question": question,
        "answer": result["final_answer"],
        "vector_context": result.get("vector_context", []),
        "graph_context": result.get("graph_context", []),
        "retrieved_doc_ids": result.get("retrieved_doc_ids", []),
        "retrieval_time": retrieval_time,
        "generation_time": generation_time,
        "total_time": total_time
    }
